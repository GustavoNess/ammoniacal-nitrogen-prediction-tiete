# ammoniacal-nitrogen-prediction-tiete
Machine learning models (Random Forest, XGBoost, and MLP) for ammoniacal nitrogen prediction to support water-quality monitoring in the TietÃª River (CETESB station TIET04200).
# Ammoniacal Nitrogen Prediction (TietÃª River) â€” RF, XGBoost, MLP

Machine learning models to **predict ammoniacal nitrogen (NHâ‚ƒ/NHâ‚„âº)** at the **TIET04200** monitoring station (TietÃª River, UGRHI 6 â€“ Upper TietÃª Basin, Brazil) using **CETESB historical data (2019â€“2025)**, under a **leakage-free temporal pipeline** and **time-aware validation**.

> Beyond prediction/nowcasting support for water-quality monitoring, the framework can also work as an **additional QA/QC layer**, flagging potentially inconsistent records using large residuals and their temporal behavior.

---

## ğŸ“Œ Project goals

- Predict ammoniacal nitrogen (mg/L) in an urban, impacted river reach (TIET04200).
- Compare **Random Forest (RF)**, **XGBoost**, and **Multilayer Perceptron (MLP)** under the same temporal protocol.
- Evaluate performance using **RÂ², RMSE, MAE, MAPE, and CCC**.
- Simulate operational behavior using **dynamic backtesting** on the most recent samples.

---

## ğŸŒ Study area and data

- **Station:** TIET04200 (Ponte dos RemÃ©dios, Marginal TietÃª), SÃ£o Paulo Metropolitan Region (RMSP).
- **Basin:** UGRHI 6 â€“ Alto TietÃª (Upper TietÃª Basin).
- **Data source:** CETESB (InfoÃguas portal) â€” public historical monitoring data (Excel).
- **Training window (modeling):** 2019â€“2025  
- **Seasonality analysis (exploratory):** 2010â€“2025 (used only for seasonal plots/tests in the paper)

---

## ğŸ§  Models evaluated

- **Random Forest (RF)** â€” ensemble of decision trees (Breiman, 2001)
- **XGBoost** â€” gradient boosted trees (Chen & Guestrin, 2016)
- **MLP** â€” feedforward neural network (Rumelhart et al., 1986)

**Hyperparameters (paper):**
- **MLP:** `hidden_layer_sizes=(100, 50)`, `max_iter=2000`, `random_state=42`
- **XGBoost:** `n_estimators=500`, `learning_rate=0.05`, `max_depth=5`,  
  `subsample=0.9`, `colsample_bytree=0.9`, `reg_lambda=1.0`, `objective=reg:squarederror`, `random_state=42`
- **RF:** `n_estimators=400`, `random_state=42`, `n_jobs=-1`

---

## ğŸ§¼ Leakage-free temporal pipeline

All data-driven steps are **fitted only on the training split within each temporal fold** (no future leakage):

1. **Cleaning** (invalid dates, duplicates, typing/unit inconsistencies)
2. **Missing values:** forward-fill; remaining NAs â†’ training median
3. **Outlier attenuation:** RobustClipper (Â±3 std per column)
4. **Seasonality features:** month + season dummies (0/1)
5. **Feature screening:** TOP_K = 5 by |Spearman Ï| with the target (computed on training only)
6. **Standardization:** fit on training, apply to test (kept for pipeline consistency)

---

## â±ï¸ Validation strategy

- **OOF temporal CV:** `TimeSeriesSplit (n=5)` (forward chaining)
- **Dynamic backtesting (operational simulation):**
  - **Recent window:** last 10 samples (**2023-02-08 â†’ 2025-05-06**)
  - **Extended window (stress test):** 16 samples (**2021-09-22 â†’ 2025-05-06**)

---

## ğŸ“Š Main results (from the paper)

### Temporal CV (OOF, 2019â€“2025)

| Model | RÂ² | RMSE (mg/L) | MAE (mg/L) | MAPE (%) | CCC |
|---|---:|---:|---:|---:|---:|
| Random Forest | 0.8361 | 2.8320 | 2.1320 | 13.7710 | 0.9044 |
| XGBoost | 0.8309 | 2.8761 | 2.2848 | 15.4218 | 0.9069 |
| MLP | 0.8113 | 3.0384 | 2.4218 | 21.5155 | 0.9103 |

### Dynamic backtesting â€” last 10 samples (2023-02-08 â†’ 2025-05-06)

| Model | RÂ² | RMSE (mg/L) | MAE (mg/L) | MAPE (%) | CCC |
|---|---:|---:|---:|---:|---:|
| Random Forest | 0.9130 | 2.1344 | 1.8360 | 14.8849 | 0.9524 |
| XGBoost | 0.9024 | 2.2603 | 1.9102 | 14.0917 | 0.9496 |
| MLP | 0.7337 | 3.7341 | 3.1696 | 38.0493 | 0.8368 |

### Dynamic backtesting â€” extended window (2021-09-22 â†’ 2025-05-06)

| Model | RÂ² | RMSE (mg/L) | MAE (mg/L) | MAPE (%) | CCC |
|---|---:|---:|---:|---:|---:|
| MLP | 0.7973 | 3.2200 | 2.8131 | 26.5992 | 0.8853 |
| Random Forest | 0.7438 | 3.6199 | 2.8211 | 16.1055 | 0.8370 |
| XGBoost | 0.7268 | 3.7380 | 3.0141 | 17.1075 | 0.8442 |

**Key takeaway:** RF and XGBoost are strongest in the **recent operational window**, while the extended window highlights **temporal regime sensitivity** (concept drift).

---

## ğŸ§ª Metrics

- **MAE** (Mean Absolute Error)
- **MSE** and **RMSE**
- **RÂ²** (Coefficient of Determination)
- **MAPE** (Mean Absolute Percentage Error)
- **CCC** (Concordance Correlation Coefficient)

---

## ğŸ—‚ï¸ Suggested repository structure

> Adjust names to match your current files.

```text
.
â”œâ”€ data/
â”‚  â”œâ”€ raw/            # CETESB exports (do not commit if large)
â”‚  â””â”€ processed/      # cleaned tables used by training
â”œâ”€ notebooks/         # EDA, correlation plots, seasonal analysis
â”œâ”€ src/
â”‚  â”œâ”€ preprocessing.py
â”‚  â”œâ”€ features.py
â”‚  â”œâ”€ train.py
â”‚  â”œâ”€ evaluate.py
â”‚  â””â”€ backtesting.py
â”œâ”€ outputs/
â”‚  â”œâ”€ figures/
â”‚  â””â”€ tables/
â”œâ”€ requirements.txt
â””â”€ README.md

##Recommended packages

numpy, pandas

scikit-learn

xgboost

matplotlib (and seaborn if you use it in notebooks)

(optional) tensorflow/keras if you run NN experiments


##â–¶ï¸ How to run (typical workflow)

Put CETESB/InfoÃguas export in data/raw/

Run preprocessing:

python -m src.preprocessing


Train + temporal CV:

python -m src.train --model rf
python -m src.train --model xgb
python -m src.train --model mlp


Dynamic backtesting:

python -m src.backtesting --window recent
python -m src.backtesting --window extended


Generate figures/tables:

python -m src.evaluate


## âœ… QA/QC use-case (optional but recommended)

This framework can be used as an additional data-quality checkpoint:

flag samples with large residuals (observed âˆ’ predicted),

prioritize retesting / audits (unit/typing errors, contamination, preservation issues, analytical interferences),

track residual drift over time as a sentinel of process bias.


##ğŸ“ Citation

If you use this repository, please cite the related manuscript:

Pacheco, G. N.; Silva, J. C.; Andrade, R. C.; Silva Filho, P. A.
Machine learning for ammoniacal nitrogen prediction to support water-quality monitoring in the TietÃª River: Random Forest, XGBoost, and Multilayer Perceptron.


##ğŸ‘¤ Authors

Gustavo Nunes Pacheco (main author)

JÃºlio CÃ©sar da Silva (advisor)

Rosane Cristina de Andrade (text review + figures)

Pedro Alves da Silva Filho (text review)

##ğŸ“„ License

Choose a license (e.g., MIT, Apache-2.0) and add a LICENSE file.
If data redistribution is restricted, keep raw data out of the repo and document how to obtain it.



##ğŸ“¬ Contact

Gustavo Nunes Pacheco
Email: gustavoo.np@hotmail.com


